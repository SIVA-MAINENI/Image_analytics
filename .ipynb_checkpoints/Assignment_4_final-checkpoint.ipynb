{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807d12a0",
   "metadata": {
    "id": "807d12a0"
   },
   "source": [
    "## Team Members\n",
    "\n",
    "1. Siva Sai Kumari Maineni\n",
    "2. Mahathi Mandapati\n",
    "3. Yang Wang\n",
    "4. Kang (Kevin) Hou\n",
    "5. Kinnary Uday Panchal\n",
    "6. Aboorva Erode Baskaran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "__xTBdtQ2zVG",
   "metadata": {
    "id": "__xTBdtQ2zVG"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff289e5",
   "metadata": {
    "id": "cff289e5"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3.6\n",
    "#Install LDA library if not already installed\n",
    "# pip3.6 install --user lda\n",
    "# the input data file is natgeo_labels_2020.xlsx (a 2-column file with id as column 1 and text in column 2)\n",
    "# there are two output files: topic_word_dist.xlsx and document_topic_dist.xlsx\n",
    "# the script prompts for the name of the columns -- in the data file, I named them id and labels. You are then prompted for # topics. 4 or 5 may be a good starting point.\n",
    "\n",
    "\n",
    "\n",
    "import os, csv, nltk, json, lda\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer,word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/kinnarypanchal/Downloads/uda-assignment-4-96341864fcec.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TCYQDFFQ9hmG",
   "metadata": {
    "id": "TCYQDFFQ9hmG"
   },
   "source": [
    "**Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOA3-lnv9dM2",
   "metadata": {
    "id": "JOA3-lnv9dM2"
   },
   "source": [
    "##### **We have used the online scrapper call phantombuster to scrape the NatGeo insta page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dm4JK3Qx-CrG",
   "metadata": {
    "id": "Dm4JK3Qx-CrG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('insta_post.json')\n",
    "df_photo =  df[df['type'] == 'Photo']\n",
    "# df_photo = df_photo.drop_duplicates(subset='postUrl', keep=\"first\")\n",
    "new_img_list =  df_photo['postUrl']\n",
    "\n",
    "# This step enable us to access the image after the Instagram url expiration date\n",
    "imgUrl_list =  [s + 'media/?size=l' for s in new_img_list]\n",
    "len(imgUrl_list)\n",
    "new_url_df = pd.DataFrame(imgUrl_list, columns=['url'])\n",
    "df_photo['imgUrl'] = new_url_df['url'].values\n",
    "\n",
    "df_photo.to_json('insta_post1.json', orient= 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zjOxeHWk25kA",
   "metadata": {
    "id": "zjOxeHWk25kA"
   },
   "source": [
    "**Image Labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb6be",
   "metadata": {
    "id": "d7beb6be"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('insta_post1.json')\n",
    "# returns JSON object as a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ce336",
   "metadata": {
    "id": "db3ce336"
   },
   "outputs": [],
   "source": [
    "def detect_labels_uri(uri):\n",
    "    \"\"\"Detects labels in the file located in Google Cloud Storage or on the\n",
    "    Web.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    image = vision.Image()\n",
    "    image.source.image_uri = uri\n",
    "\n",
    "    response = client.label_detection(image=image)\n",
    "    labels = response.label_annotations\n",
    "    \n",
    "    lbs = list()\n",
    "    scr = list()\n",
    "    for label in labels:\n",
    "        lbs.append(label.description)\n",
    "        scr.append(label.score)\n",
    "        \n",
    "        \n",
    "    return (lbs, scr)\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "        \n",
    "pid = list()\n",
    "for i in range(0,len(data)):\n",
    "    pid.append(data[i]['postId'])\n",
    "\n",
    "items = Counter(pid).keys()\n",
    "print(\"No of unique items in the list are:\", len(items))\n",
    "\n",
    "url = list()\n",
    "for i in range(0, len(data)):\n",
    "    url.append(data[i]['imgUrl'])\n",
    "\n",
    "items = Counter(url).keys()\n",
    "print(\"No of unique items in the list are:\", len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3b195",
   "metadata": {
    "id": "8ba3b195"
   },
   "outputs": [],
   "source": [
    "url = list()\n",
    "for i in range(0, len(data)):\n",
    "    url.append(data[i]['imgUrl'])\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    lbs, scr = detect_labels_uri(url[i])\n",
    "    data[i]['labels'] = lbs \n",
    "    data[i]['score'] = scr\n",
    "\n",
    "with open('insta_label_score.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccaa50a",
   "metadata": {
    "id": "fccaa50a"
   },
   "source": [
    "### There are 675 different posts with 893 images in total. \n",
    "### However there are only 485 unique images used throughout all the 675 posts\n",
    "### This implies multiple posts have same images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvajd5Bb4R1i",
   "metadata": {
    "id": "Pvajd5Bb4R1i"
   },
   "source": [
    "**Data Preperation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8d4c2",
   "metadata": {
    "id": "3ae8d4c2"
   },
   "outputs": [],
   "source": [
    "# reading data\n",
    "\n",
    "f1 = open('insta_label_score.json')\n",
    "dt = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa16b56",
   "metadata": {
    "id": "8aa16b56"
   },
   "outputs": [],
   "source": [
    "lbs = list()\n",
    "scr = list()\n",
    "likeCount = list()\n",
    "commentCount = list()\n",
    "caption = list()\n",
    "for i in range(0, len(data)):\n",
    "    lbs.append(dt[i]['labels'])\n",
    "    scr.append(dt[i]['score'])\n",
    "    likeCount.append(dt[i]['likeCount'])\n",
    "    commentCount.append(dt[i]['commentCount'])\n",
    "    caption.append(dt[i]['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a81898",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "19a81898",
    "outputId": "766c2713-77ab-4d60-e0bf-68648bc55abe"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(url, lbs, scr, likeCount, commentCount, caption)), columns =['imgUrl','labels', 'score', 'likeCount', 'commentCount', 'caption']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff213c9",
   "metadata": {
    "id": "8ff213c9"
   },
   "outputs": [],
   "source": [
    "df.to_csv('img_lbs_scr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L0sdzrOq-bof",
   "metadata": {
    "id": "L0sdzrOq-bof"
   },
   "source": [
    "**Creating the Engagement Column(Target)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83e674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "9c83e674",
    "outputId": "6b12262f-6c80-40c3-e038-fbe9eb28d244"
   },
   "outputs": [],
   "source": [
    "df['normalizedLikeCount'] = df['likeCount']/(df['likeCount'].max())\n",
    "df['normalizedCommentCount'] = df['commentCount']/(df['commentCount'].max())\n",
    "df['engagementScore'] =  .4*df['normalizedLikeCount'] + .6*df['normalizedCommentCount']\n",
    "df['engagement'] = df.apply(lambda x: 1 if x['engagementScore'] > df['engagementScore'].median() else 0, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b7218",
   "metadata": {
    "id": "148b7218"
   },
   "outputs": [],
   "source": [
    "df.to_csv('img_lbs_scr_with_engagement.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jHe2JTPP9QWI",
   "metadata": {
    "id": "jHe2JTPP9QWI"
   },
   "source": [
    "## Task C - Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qg4DCkkE6EH5",
   "metadata": {
    "id": "qg4DCkkE6EH5"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(version_desc):\n",
    "    lowercase=version_desc.lower()\n",
    "    text = wordnet_lemmatizer.lemmatize(lowercase)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q5UbSZKk9_Ec",
   "metadata": {
    "id": "q5UbSZKk9_Ec"
   },
   "outputs": [],
   "source": [
    "image_df = pd.read_csv(\"img_lbs_scr_with_engagement.csv\")\n",
    "image_df.reset_index(drop=True)\n",
    "headerList = ['id', 'imgUrl', 'labels', 'score', 'likeCount', 'commentCount', 'caption', 'normalizedLikeCount', 'normalizedCommentCount', 'engagementScore', 'engagement']\n",
    "image_df.to_csv(\"img_lbs_scr_with_engagement_mod.csv\", header=headerList, index=False)\n",
    "image_df = pd.read_csv(\"img_lbs_scr_with_engagement_mod.csv\")\n",
    "\n",
    "\n",
    "#checking for nulls if present any\n",
    "print(\"Number of rows with any of the empty columns:\")\n",
    "print(image_df.isnull().sum().sum())\n",
    "reviews_df = image_df.dropna()\n",
    "\n",
    "\n",
    "id_column = input('provide the column name for id: ')\n",
    "labels_column = input('provide the column name for text: ')\n",
    "ntopics= input('Provide the number of latent topics: ');\n",
    "\n",
    "\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(reviews_df[labels_column])\n",
    "\n",
    "print(total_features_words.shape)\n",
    "\n",
    "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)\n",
    "lda_train = model.fit(total_features_words)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "image_df = image_df.join(doc_topic)\n",
    "doc_topic_df = pd.DataFrame()\n",
    "\n",
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    doc_topic_df[topic]=reviews_df.groupby([id_column])[i].mean()\n",
    "\n",
    "topic_words_df = doc_topic_df.reset_index()\n",
    "topics = pd.DataFrame(topic_word)\n",
    "topics.columns = vec_words.get_feature_names()\n",
    "topics1 = topics.transpose()\n",
    "\n",
    "\n",
    "print (\"Topics word distribution written in file topic_word_dist.xlsx \")\n",
    "topics1.to_excel(\"topic_word_dist.xlsx\")\n",
    "doc_topic_df.to_excel(\"document_topic_dist.xlsx\",index=False)\n",
    "print (\"Document topic distribution written in file document_topic_dist.xlsx \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r3ROja6HM9at",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "r3ROja6HM9at",
    "outputId": "547863fe-a4c1-4707-8a59-09b4743542db"
   },
   "outputs": [],
   "source": [
    "doc_topic_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0O76Ed72-_g4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "0O76Ed72-_g4",
    "outputId": "276570b3-0101-4068-e9f8-398fe2e180c5"
   },
   "outputs": [],
   "source": [
    "df_nat_final = pd.concat([reviews_df.reset_index(drop=True), doc_topic_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "q1=np.percentile(df_nat_final.engagementScore, 25) \n",
    "q2=np.percentile(df_nat_final.engagementScore, 50)  \n",
    "q3=np.percentile(df_nat_final.engagementScore, 75)\n",
    "\n",
    "top_quartile=df_nat_final[df_nat_final['engagementScore']>q3]\n",
    "top_quartile.count()\n",
    "\n",
    "average_topic_weights_top = top_quartile[[\"topic_0\",'topic_1',\"topic_2\",'topic_3',\"topic_4\",'topic_5',\"topic_6\",'topic_7', 'topic_8', 'topic_9']].mean(axis=0)\n",
    "\n",
    "bottom_quartile=df_nat_final[df_nat_final['engagementScore']<q1]\n",
    "bottom_quartile.count()\n",
    "\n",
    "average_topic_weights_bot = bottom_quartile[[\"topic_0\",'topic_1',\"topic_2\",'topic_3',\"topic_4\",'topic_5',\"topic_6\",'topic_7', 'topic_8', 'topic_9']].mean(axis=0)\n",
    "\n",
    "quartile_topics = pd.concat([average_topic_weights_top,average_topic_weights_bot],axis=1)\n",
    "quartile_topics.columns = ['Top Quartile','Bottom Quartile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ee0wDCUF-1FX",
   "metadata": {
    "id": "Ee0wDCUF-1FX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Gcu9us_-1B9",
   "metadata": {
    "id": "7Gcu9us_-1B9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4AWLRVE9-1qT",
   "metadata": {
    "id": "4AWLRVE9-1qT"
   },
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yJbZPuFz_k42",
   "metadata": {
    "id": "yJbZPuFz_k42"
   },
   "source": [
    "**Reading the Labeled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538qOuVJ-3dU",
   "metadata": {
    "id": "538qOuVJ-3dU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('img_lbs_scr_with_engagement.csv') \\\n",
    "       .drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df['imgUrl'] = df['imgUrl'].apply(lambda x: x.split('media')[0])\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(columns=['img_url', 'description'])\n",
    "# Opening JSON file\n",
    "f = open('insta_post1.json')  \n",
    "# returns JSON object as  a dictionary\n",
    "data = json.load(f)\n",
    "# Iterating through the json list\n",
    "for descr in data:\n",
    "    df2 = df2.append({'img_url': descr['postUrl'], 'description': descr['description'] }, ignore_index=True)\n",
    "f.close()\n",
    "\n",
    "print(df.shape, df2.shape)\n",
    "\n",
    "descr_lookup = df2.set_index('img_url').to_dict()['description']\n",
    "df['description'] = df.apply(lambda row: descr_lookup[row['imgUrl']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9r5irt4_veF",
   "metadata": {
    "id": "q9r5irt4_veF"
   },
   "source": [
    "**Processing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iF6L9W06_pUG",
   "metadata": {
    "id": "iF6L9W06_pUG"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    \"\"\"\n",
    "    function to remove punctuations in a \n",
    "    sentence\n",
    "    \"\"\"\n",
    "    string_punctuation = '''()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "    no_punct = \"\"\n",
    "    for letter in s:\n",
    "        if letter not in string_punctuation:\n",
    "            no_punct += letter\n",
    "    return no_punct\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def lemmetization(pos_tagged_words):\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tagged_words:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# removing unnecessary characters\n",
    "df['description'] = df['description'].apply(lambda x: x.lower().split('|')[-1] \\\n",
    "                                                       .replace('\\n', '') \\\n",
    "                                                       .replace('“', '') \\\n",
    "                                                       .replace('”', ''))\n",
    "# removing punctuations in a sentence\n",
    "i = 0\n",
    "for row in df['description']:\n",
    "    df.iloc[ i, -1] = remove_punctuation(row)\n",
    "    i=i+1\n",
    "    \n",
    "df['description'] = df['description'].apply(word_tokenize)\n",
    "df['description'] = df['description'].apply(lambda x : [w for w in x if w not in stop])\n",
    "\n",
    "# df['description'] = df['description'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df['description'] = df['description'].apply(lambda x : \" \".join(x))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['description'] = df['description'].apply(lambda x: nltk.pos_tag(x))\n",
    "df['description'] = df['description'].apply(lambda words: list(map(lambda x: (x[0], pos_tagger(x[1])), words)))\n",
    "df['description'] = df['description'].apply(lambda pos_tagged_words: lemmetization(pos_tagged_words))\n",
    "\n",
    "\n",
    "Text_Column = list(df['description'].copy())\n",
    "sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(Text_Column)\n",
    "Tfidf_Output = pd.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names())\n",
    "\n",
    "final_data = pd.concat([Tfidf_Output, df['engagement']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "URPw1nM5_yq1",
   "metadata": {
    "id": "URPw1nM5_yq1"
   },
   "outputs": [],
   "source": [
    "#Plotting Confusion Matrix\n",
    "def show_confusion_matrix(C,class_labels=['0','1']):\n",
    "\n",
    "    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n",
    "\n",
    "    # true negative, false positive, etc...\n",
    "    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n",
    "\n",
    "    NP = fn+tp # Num positive examples\n",
    "    NN = tn+fp # Num negative examples\n",
    "    N  = NP+NN\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax  = fig.add_subplot(111)\n",
    "    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n",
    "\n",
    "    # Draw the grid boxes\n",
    "    ax.set_xlim(-0.5,2.5)\n",
    "    ax.set_ylim(2.5,-0.5)\n",
    "    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n",
    "    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n",
    "    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n",
    "    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n",
    "\n",
    "    # Set xlabels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "    ax.set_xticks([0,1,2])\n",
    "    ax.set_xticklabels(class_labels + [''])\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    # These coordinate might require some tinkering. Ditto for y, below.\n",
    "    ax.xaxis.set_label_coords(0.34,1.06)\n",
    "\n",
    "    # Set ylabels\n",
    "    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n",
    "    ax.set_yticklabels(class_labels + [''],rotation=90)\n",
    "    ax.set_yticks([0,1,2])\n",
    "    ax.yaxis.set_label_coords(-0.09,0.65)\n",
    "\n",
    "\n",
    "    # Fill in initial metrics: tp, tn, etc...\n",
    "    ax.text(0,0,\n",
    "            '%d'%(tn),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,1,\n",
    "            '%d'%fn,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,0,\n",
    "            '%d'%fp,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    ax.text(1,1,\n",
    "            '%d'%(tp),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    # Fill in secondary metrics: accuracy, true pos rate, etc...\n",
    "    ax.text(2,0,\n",
    "            'Error: %.2f'%(fp / (fp+tn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,1,\n",
    "            'Error: %.2f'%(fn / (tp+fn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,2,\n",
    "            'Accuracy: %.2f'%((tp+tn+0.)/N),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,2,' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,2,\n",
    "            ' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkyNDoXeALEw",
   "metadata": {
    "id": "RkyNDoXeALEw"
   },
   "source": [
    "**With Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bn7FVsFA_131",
   "metadata": {
    "id": "bn7FVsFA_131"
   },
   "outputs": [],
   "source": [
    "selected_features = [col for col in final_data.columns if col not in ['engagement']]\n",
    "X = final_data[selected_features]\n",
    "Y = final_data['engagement']\n",
    "\n",
    "#Logit Regression\n",
    "classifier = LogisticRegression()\n",
    "Y_pred = model_selection.cross_val_predict(classifier, X, Y, cv=5)\n",
    "\n",
    "# calculating CV accuracy\n",
    "cv_scores = cross_val_score(classifier, X, Y, cv=5)\n",
    "print(\"CV  accuracy\", sum(cv_scores) / len(cv_scores))\n",
    "\n",
    "# calculating CV roc_roc\n",
    "cv_scores = cross_val_score(classifier, X, Y, cv=5, scoring='roc_auc')\n",
    "print(\"CV auc score\", sum(cv_scores) / len(cv_scores))\n",
    "\n",
    "# calculating confusion matrix\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "show_confusion_matrix(cm, ['0', '1'])\n",
    "plt.show()\n",
    "plt.savefig('Confusion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gge67FL2ADGe",
   "metadata": {
    "id": "gge67FL2ADGe"
   },
   "source": [
    "**With Image Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ut0UCggkAHsp",
   "metadata": {
    "id": "Ut0UCggkAHsp"
   },
   "outputs": [],
   "source": [
    "df['labels']=df['labels'].str.split(',').apply(lambda x : (\"\".join(x)).replace(\"'\",''))\n",
    "df['labels']=df['labels'].str[1:-1]\n",
    "Text_Column=df['labels']\n",
    "sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(Text_Column)\n",
    "Tfidf_Output = pd.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names())\n",
    "\n",
    "\n",
    "Input = pd.concat([df, Tfidf_Output], axis=1)\n",
    "Input=Input.drop(['imgUrl', 'labels', 'score', 'likeCount', 'commentCount', 'caption',\n",
    "       'normalizedLikeCount', 'normalizedCommentCount', 'engagementScore','description'], axis=1)\n",
    "\n",
    "X = Input.loc[:, Input.columns != 'engagement']\n",
    "Y = Input['engagement']\n",
    "classifier = LogisticRegression()\n",
    "Y_pred = cross_val_predict(classifier, X, Y, cv=5)\n",
    "confusion_matrix = confusion_matrix((np.array(Y)), Y_pred)\n",
    "\n",
    "show_confusion_matrix(confusion_matrix, ['0', '1'])\n",
    "plt.show()\n",
    "plt.savefig('Confusion_matrix_for_Description_model.png')\n",
    "\n",
    "print(roc_auc_score(Y, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NkANfcybAxhQ",
   "metadata": {
    "id": "NkANfcybAxhQ"
   },
   "source": [
    "**using both labels and description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lwlbmA79AVqo",
   "metadata": {
    "id": "lwlbmA79AVqo"
   },
   "outputs": [],
   "source": [
    "df3=pd.read_csv('data_for_caption_logit.csv')\n",
    "df3=df3.drop(['Unnamed: 0','engagement'], axis=1)\n",
    "\n",
    "Input2= pd.concat([Input, df3], axis=1)\n",
    "X2 = Input2.loc[:, Input2.columns != 'engagement']\n",
    "Y2 = Input2['engagement']\n",
    "classifier = LogisticRegression()\n",
    "Y_pred2 = model_selection.cross_val_predict(classifier, X2, Y2, cv=5)\n",
    "confusion_matrix = confusion_matrix((np.array(Y2)), Y_pred2)\n",
    "print(confusion_matrix)\n",
    "\n",
    "show_confusion_matrix(confusion_matrix, ['0', '1'])\n",
    "plt.show()\n",
    "plt.savefig('Confusion_for_model_with_descr_imglabels.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
